---
title: "MovieLens"
author: "Aditya Taktode - Harvard Data Science Professional - Capstone"
date: "20/06/2020"
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    highlights: pygments
    keep_tex: true
---

```{r setup, include=T}
knitr::opts_chunk$set(echo = T, fig.align = 'center', cache = F, cache.lazy = F)
```

# Executive Summary

The purpose of this project is creating a recommender system using the MovieLens dataset.

The version of movielens dataset used for this final assignment contains approximately 10 Million movie ratings, divided in 9 Milion for training and one Milion for validation. 

It is a small subset of a much larger (and famous) dataset with several millions of ratings.

After a initial data exploration, the different recommender systems built on this dataset are evaluated and choosen based on the RMSE (Root Mean Squared Error) that should be at least lower than **0.87750**.

$$\mbox{RMSE} = \sqrt{\frac{1}{n}\sum_{t=1}^{n}e_t^2}$$

# Getting set-up with the required libraries

```{r Code.R, warning=FALSE, message=FALSE, include=FALSE}
#Install all the needded libraries if not pressent already
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org") 
if(!require(rpart.plot)) install.packages("rpart.plot", repos = "http://cran.us.r-project.org") 
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org") 
if(!require(gbm)) install.packages("gbm", repos = "http://cran.us.r-project.org") 
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org") 
if(!require(biglm)) install.packages("biglm", repos = "http://cran.us.r-project.org") 
if(!require(broom)) install.packages("broom", repos = "http://cran.us.r-project.org") 
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org") 
if(!require(recommenderlab)) install.packages("recommenderlab", repos = "http://cran.us.r-project.org") 
if(!require(recosystem)) install.packages("recosystem", repos = "http://cran.us.r-project.org")

#Loading all the required libraries
library(tidyverse) 
library(ggplot2) 
library(caret) 
library(rpart) 
library(rpart.plot) 
library(randomForest) 
library(gbm) 
library(stringr) 
library(biglm) 
library(broom) 
library(data.table) 
library(lubridate) 
library(recommenderlab) 
library(recosystem)
```


# Load and prepare the data 

The 10 Million movielens dataset is divided into two sets: ```dataset``` for training purpose and ```validation``` for validation (i.e. final test) purpose.

```{r, include=TRUE, echo=TRUE}
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
dataset <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in dataset set
validation <- temp %>% 
  semi_join(dataset, by = "movieId") %>%
  semi_join(dataset, by = "userId")

# Add rows removed from validation set back into dataset set
removed <- anti_join(temp, validation)
dataset <- rbind(dataset, removed)
```

# Data Exploration

## Preliminary data exploration

```{r, echo = TRUE, include=TRUE}
#Summary of dataset
glimpse(dataset)
```

The features in dataset are six:

- **userId** ```<integer>``` that contains the unique identification number for each user.
- **movieId** ```<numeric>``` that contains the unique identification number for each movie.
- **rating** ```<numeric>``` that contains the rating of one movie by one user. Ratings are made on a 5-Star scale with half-star increments.
- **timestamp** ```<integer>``` that contains the timestamp for one specific rating provided by one user.
- **title** ```<character>``` that contains the title of each movie including the year of the release.
- **genres** ```<character>``` that contains a list of pipe-separated of genre of each movie.


After having a glimpse of summary of our dataset, it has come to notice that the ```genres``` are pipe-separated values.

```{r, echo = TRUE, include=TRUE}
#peeking into 'genres'
head(dataset$genres, n = 20)
```


It is necessary to extract them for better, robust and precise estimation.

```{r, echo = TRUE, include=TRUE}
#separate rows for extracting different genres
dataset <- dataset %>% separate_rows(genres, sep = "\\|")

#the new genres are
levels(factor(dataset$genres))
```
The new genres look better and well segregated.

We'll have to do the same for validation set as well

```{r, echo=TRUE, include=TRUE}
#separate rows for extracting different genres
validation <- validation %>% separate_rows(genres, sep = "\\|")
```

```{r, echo = TRUE, include=TRUE}
#check for any missing values
sum(is.na(dataset))
```

So, there are no missing values, cheers!!

Now, diving into the number of unique values for some features we've got

```{r, echo=TRUE, include=TRUE}

#total number of unique users
n_distinct(dataset$userId)

#total number of unique movies
n_distinct(dataset$movieId)

#total number of unique genres
n_distinct(dataset$genres)
```

# Ratings explorating analysis

### Rating distribution exploration

```{r, echo=TRUE, include=TRUE}
dataset %>% group_by(rating) %>% summarise(count = n()) %>% ggplot(aes(rating, count)) + geom_point() + geom_line() + labs(title = "Rating counts per rating", y = "Count", x = "Rating")
```
This visualizationg shows that there is a small amount of negative votes(i.e. below 3). Maybe, the users give ratings only if they like it.
Also, half-star ratings are less likely as compared to the full-star ratings.

### Overview of rating frequency through years

```{r, echo=TRUE, include=TRUE}
dataset %>% mutate(year = year(as_datetime(timestamp, origin = '1970-01-01'))) %>% ggplot(aes(x = year)) + geom_histogram() + title("Rating frequency through years")
```

This visualizes frequench of user's ratings through years

### Number of ratings given to a movie

```{r, echo=TRUE, include=TRUE}
dataset %>% group_by(movieId) %>% summarise(count = n()) %>% ggplot(aes(count)) + geom_histogram(col = "white") + scale_x_log10() + ggtitle("number of ratings given to a single movie") + xlab("Number of Ratings") + ylab("Number of Movies")
```
This seems normally distributed (given logarithmic transformation taken on X-axis i.e. for number of ratings)
Lets have a leav of faith and hope that this would be of some help in building the model ;)

### Rating distribution per user

```{r, echo=TRUE, include=TRUE}
dataset %>% group_by(userId) %>% summarise(count = n()) %>% ggplot(aes(count)) + geom_histogram(color = 'white') + scale_x_log10() + xlab("Number of ratings given") + ylab("Number of users") + ggtitle("Number of ratings given to the movies by the users")
```

## Genre analysis

### Overview of rating distribution over different genres

```{r, echo=TRUE, include=TRUE}
dataset %>% group_by(genres) %>% summarise(count = n()) %>% ggplot(aes(genres, count)) + geom_col() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
This gives an overview of rating distribution over different genres.


# Analysis - Model building and evaluation

















